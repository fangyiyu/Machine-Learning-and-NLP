{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.classes = np.unique(y)\n",
    "        self.parameters = {}\n",
    "        for i, c in enumerate(self.classes):\n",
    "            # Calculate the mean, variance and the prior possibily for all the classes\n",
    "            X_index_c = X[np.where(y == c)]\n",
    "            X_index_c_mean = np.mean(X_index_c, axis = 0, keepdims = True)\n",
    "            X_index_c_var = np.var(X_index_c, axis = 0, keepdims = True)\n",
    "            parameters = {'mean':X_index_c_mean, 'var': X_index_c_var, 'prior': X_index_c.shape[0]/X.shape[0]}\n",
    "            self.parameters['class'+str(c)] = parameters\n",
    "    \n",
    "    def likelihood(self, X, classes):\n",
    "        # Calculate the probability density function\n",
    "        # To avoid the denominator == 0, add eps\n",
    "        eps = 1e-4\n",
    "        mean = self.parameters['class'+str(classes)]['mean']\n",
    "        var = self.parameters['class'+str(classes)]['var']\n",
    "        \n",
    "        numerator = np.exp(-(X-mean)**2/(2*var+eps))\n",
    "        denominator = np.sqrt(2*np.pi*var+eps)\n",
    "        \n",
    "        # Assume all features are independent,logarithm for adding instead of multiplying\n",
    "        result = np.sum(np.log(numerator/denominator), axis = 1, keepdims = True)\n",
    "        \n",
    "        return result.T\n",
    "    \n",
    "    def posterior(self, X):\n",
    "        # Calculate the possibility of each class P(Y|x1,x2,x3) =  P(Y)*P(x1|Y)*P(x2|Y)*P(x3|Y)\n",
    "        output = []\n",
    "        for y in range(self.classes.shape[0]):\n",
    "            prior = np.log(self.parameters['class'+str(y)]['prior'])\n",
    "            output.append(prior + self.likelihood(X, y))\n",
    "        return output\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Find the class with the highest probability as the output class\n",
    "        output = self.posterior(X)\n",
    "        output = np.reshape(output, (self.classes.shape[0], X.shape[0]))\n",
    "        prediction = np.argmax(output, axis = 0)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 0.98\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data = datasets.load_iris()\n",
    "    X = preprocessing.normalize(data.data)\n",
    "    y = data.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "    clf = NaiveBayes()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('The accuracy is:', accuracy)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
